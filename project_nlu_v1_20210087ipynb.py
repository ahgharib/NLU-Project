# -*- coding: utf-8 -*-
"""project_NlU_V1_20210087ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A1BoAeDJlzUqWbaeFSfE5oJwJCsmxt_u
"""

pip install transformers peft datasets accelerate bitsandbytes

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

tokenizer = AutoTokenizer.from_pretrained("gpt2-large")
model = AutoModelForCausalLM.from_pretrained("gpt2-large")

tokenizer.pad_token = tokenizer.eos_token

device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)

input_text = "The future of education is"
inputs = tokenizer(input_text, return_tensors="pt").to(device)
outputs = model.generate(**inputs, max_length=50)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))

from peft import LoraConfig, get_peft_model, TaskType

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["c_attn"],
    lora_dropout=0.1,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d thevirusx3/automated-essay-scoring-dataset

!unzip automated-essay-scoring-dataset.zip

import pandas as pd

df = pd.read_csv('/content/test_set.tsv', encoding="latin1", sep="\t")
df.head()

from datasets import Dataset

dataset = Dataset.from_pandas(df[['essay']])

def tokenize_function(examples):
    tokens = tokenizer(examples["essay"], truncation=True, padding="max_length", max_length=512)
    tokens["labels"] = tokens["input_ids"].copy()
    return tokens

tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=["essay"])

import transformers
print(transformers.__version__)

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./gpt2-lora-finetuned",
    overwrite_output_dir=True,
    num_train_epochs=2,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    learning_rate=2e-4,
    save_steps=100,
    save_total_limit=2,
    logging_steps=10,
    fp16=True,
    report_to="none"
)

from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
)

trainer.train()

base_model = AutoModelForCausalLM.from_pretrained("gpt2-large").to(device)
base_model.eval()

references = [
    "The essay is well-organized but needs more examples.",
    "Good structure and clear argument, but grammatical errors present.",
    "Ideas are interesting but lack supporting evidence.",
    "The essay is persuasive but could be more concise.",
    "Good grammar and vocabulary usage, needs stronger conclusion.",
    "Logical flow is good, but lacks depth in analysis.",
    "Essay responds to the prompt well but has poor paragraph transitions.",
    "Great introduction, but the body paragraphs are weak.",
    "Clear writing style but missing citations for some claims.",
    "Essay covers important points but lacks emotional impact."
]

import evaluate

bleu = evaluate.load("bleu")
rouge = evaluate.load("rouge")

base_predictions = []
fine_predictions = []

for idx in range(10):
    input_text = df['essay'][idx]

    inputs = tokenizer(input_text, return_tensors="pt", padding=True, truncation=True, max_length=512).to(device)

    with torch.no_grad():
        base_outputs = base_model.generate(**inputs, max_new_tokens=50)
    base_feedback = tokenizer.decode(base_outputs[0], skip_special_tokens=True)
    base_predictions.append(base_feedback)

    with torch.no_grad():
        fine_outputs = model.generate(**inputs, max_new_tokens=50)
    fine_feedback = tokenizer.decode(fine_outputs[0], skip_special_tokens=True)
    fine_predictions.append(fine_feedback)

bleu_base = bleu.compute(predictions=base_predictions, references=[[ref] for ref in references])
bleu_fine = bleu.compute(predictions=fine_predictions, references=[[ref] for ref in references])

rouge_base = rouge.compute(predictions=base_predictions, references=references)
rouge_fine = rouge.compute(predictions=fine_predictions, references=references)

print(f"Base BLEU: {bleu_base['bleu']*100:.2f}%")
print(f"Fine-Tuned BLEU: {bleu_fine['bleu']*100:.2f}%")

print(f"Base ROUGE-L: {rouge_base['rougeL']*100:.2f}%")
print(f"Fine-Tuned ROUGE-L: {rouge_fine['rougeL']*100:.2f}%")

import matplotlib.pyplot as plt

models = ['Base GPT-2', 'Fine-Tuned GPT-2']
bleu_scores = [bleu_base['bleu']*100, bleu_fine['bleu']*100]
rouge_scores = [rouge_base['rougeL']*100, rouge_fine['rougeL']*100]

x = range(len(models))

plt.figure(figsize=(8,6))

plt.bar([i-0.2 for i in x], bleu_scores, width=0.4, label='BLEU', align='center')
plt.bar([i+0.2 for i in x], rouge_scores, width=0.4, label='ROUGE-L', align='center')

plt.xticks(x, models)
plt.ylabel('Score (%)')
plt.title('Model Performance Comparison (BLEU and ROUGE-L)')
plt.legend()
plt.grid(True, linestyle='--', alpha=0.7)
plt.show()

import json

input_text = df['essay'][0]

inputs = tokenizer(input_text, return_tensors="pt", padding=True, truncation=True, max_length=512).to(device)
outputs = model.generate(**inputs, max_new_tokens=50)
generated_feedback = tokenizer.decode(outputs[0], skip_special_tokens=True)

structured_output = {
    "feedback": generated_feedback,
    "comments": [
        "Work on clarity and coherence.",
        "Use more specific examples."
    ],
    "score_estimation": 7
}

print(json.dumps(structured_output, indent=2))